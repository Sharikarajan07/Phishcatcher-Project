{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3343cbc3-aa07-4baa-a51e-21fbcf45f650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial class counts:\n",
      " type\n",
      "benign        428103\n",
      "defacement     96457\n",
      "phishing       94111\n",
      "malware        32520\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, math, tldextract\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# 1. Load your dataset\n",
    "df = pd.read_csv(\"malicious_phish.csv\")  # Adjust file path\n",
    "print(\"Initial class counts:\\n\", df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac52a817-b1d0-4dcd-bdbd-418036b1cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'benign': np.int64(0), 'defacement': np.int64(1), 'malware': np.int64(2), 'phishing': np.int64(3)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['type_encoded'] = le.fit_transform(df['type'])\n",
    "\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a8e33-31e5-4f5c-bc2d-26616d36e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "TRUSTED_DOMAINS = {\n",
    "    'google.com', 'youtube.com', 'facebook.com',\n",
    "    'instagram.com', 'reddit.com', 'wikipedia.org',\n",
    "    'twitter.com', 'amazon.com', 'linkedin.com',\n",
    "    'netflix.com', 'microsoft.com', 'github.com',\n",
    "    'paypal.com', 'apple.com', 'bing.com', 'chatgpt.com'\n",
    "}\n",
    "\n",
    "def extract_features_dict(url):\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        ext = tldextract.extract(url)\n",
    "        domain = ext.domain or \"\"\n",
    "        sub = ext.subdomain or \"\"\n",
    "        full = ext.top_domain_under_public_suffix.lower()  # updated\n",
    "\n",
    "        url_len = len(url)\n",
    "        specials = len(re.findall(r'[^a-zA-Z0-9]', url))\n",
    "        entropy = -sum((url.count(c)/url_len) * math.log2(url.count(c)/url_len) for c in set(url)) if url_len else 0\n",
    "\n",
    "        features = {\n",
    "            'url_length': url_len,\n",
    "            'domain_length': len(domain),\n",
    "            'path_length': len(p.path),\n",
    "            'count_dot': url.count('.'),\n",
    "            'count_hyphen': url.count('-'),\n",
    "            'count_at': url.count('@'),\n",
    "            'count_question': url.count('?'),\n",
    "            'count_equal': url.count('='),\n",
    "            'count_slash': url.count('/'),\n",
    "            'has_login': int('login' in url.lower()),\n",
    "            'has_bank': int('bank' in url.lower()),\n",
    "            'has_verify': int('verify' in url.lower()),\n",
    "            'has_ip': int(bool(re.match(r'(?:\\d{1,3}\\.){3}\\d{1,3}', p.netloc))),\n",
    "            'digit_ratio': sum(c.isdigit() for c in url) / url_len,\n",
    "            'special_char_ratio': specials / url_len,\n",
    "            'url_entropy': entropy,\n",
    "            'has_https': int(p.scheme.lower() == 'https'),\n",
    "            'subdomain_length': len(sub),\n",
    "            'count_sensitive_words': sum(w in url.lower() for w in ['login','bank','verify','secure','account','update']),\n",
    "            'is_free_hosting': int(any(h in url.lower() for h in ['000webhost','freenom','infinityfree'])),\n",
    "            'is_shortened': int(any(s in url.lower() for s in ['bit.ly','tinyurl.com','goo.gl','ow.ly','is.gd'])),\n",
    "            'brand_in_subdomain': int(any(b in sub.lower() for b in ['paypal','citi','facebook','google','amazon'])),\n",
    "            'is_trusted': int(full in TRUSTED_DOMAINS)  \n",
    "        }\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Feature extraction failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87fc0aee-9675-42eb-b2d6-d7817749bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply extractor\n",
    "feat_rows = df['url'].apply(extract_features_dict)\n",
    "feat_df = pd.DataFrame(feat_rows.tolist())\n",
    "feat_df['type_encoded'] = df['type_encoded']\n",
    "feat_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad0ce40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply extractor (23 features!)\n",
    "feat_rows = df['url'].apply(extract_features_dict)\n",
    "feat_df = pd.DataFrame(feat_rows.tolist())\n",
    "\n",
    "# If you had previously assigned labels, ensure using correct column:\n",
    "feat_df['type_encoded'] = df.loc[feat_df.index, 'type_encoded']\n",
    "\n",
    "# Drop incomplete entries\n",
    "feat_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c43abfc-1e08-4bc2-b6e5-bbe0d3605c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: type_encoded\n",
      "0    342482\n",
      "1     77165\n",
      "3     75289\n",
      "2     26016\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: type_encoded\n",
      "0    342482\n",
      "1    342482\n",
      "3    342482\n",
      "2    342482\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     85621\n",
      "           1       0.94      0.98      0.96     19292\n",
      "           2       0.97      0.86      0.91      6504\n",
      "           3       0.89      0.81      0.85     18822\n",
      "\n",
      "    accuracy                           0.95    130239\n",
      "   macro avg       0.94      0.91      0.92    130239\n",
      "weighted avg       0.95      0.95      0.95    130239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Split/train\n",
    "X = feat_df.drop(['type_encoded'], axis=1)\n",
    "y = feat_df['type_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "X_tr, y_tr = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", y_tr.value_counts())\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\"\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3a7dab2-62d9-415d-b334-ae056ef70be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "# 5. Save model\n",
    "with open(\"phishcatcher_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"✅ Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "086c14f3-62cb-4818-bd6b-789dbea3e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)  # do this again from your training notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d84cd8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url_length': 18, 'domain_length': 6, 'path_length': 0, 'count_dot': 1, 'count_hyphen': 0, 'count_at': 0, 'count_question': 0, 'count_equal': 0, 'count_slash': 2, 'has_login': 0, 'has_bank': 0, 'has_verify': 0, 'has_ip': 0, 'digit_ratio': 0.0, 'special_char_ratio': 0.2222222222222222, 'url_entropy': 3.5724312513221195, 'has_https': 1, 'subdomain_length': 0, 'count_sensitive_words': 0, 'is_free_hosting': 0, 'is_shortened': 0, 'brand_in_subdomain': 0, 'is_trusted': 1}\n"
     ]
    }
   ],
   "source": [
    "print(extract_features_dict(\"https://google.com\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e41b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url_length': 34, 'domain_length': 4, 'path_length': 0, 'count_dot': 2, 'count_hyphen': 1, 'count_at': 0, 'count_question': 0, 'count_equal': 0, 'count_slash': 2, 'has_login': 1, 'has_bank': 0, 'has_verify': 0, 'has_ip': 0, 'digit_ratio': 0.0, 'special_char_ratio': 0.17647058823529413, 'url_entropy': 4.314972767530033, 'has_https': 0, 'subdomain_length': 18, 'count_sensitive_words': 2, 'is_free_hosting': 0, 'is_shortened': 0, 'brand_in_subdomain': 1, 'is_trusted': 0}\n"
     ]
    }
   ],
   "source": [
    "print(extract_features_dict(\"http://paypalsecure-login.fake.com\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93054e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects features: 23\n",
      "Feature names in model: ['url_length', 'domain_length', 'path_length', 'count_dot', 'count_hyphen', 'count_at', 'count_question', 'count_equal', 'count_slash', 'has_login', 'has_bank', 'has_verify', 'has_ip', 'digit_ratio', 'special_char_ratio', 'url_entropy', 'has_https', 'subdomain_length', 'count_sensitive_words', 'is_free_hosting', 'is_shortened', 'brand_in_subdomain', 'is_trusted']\n"
     ]
    }
   ],
   "source": [
    "print(\"Model expects features:\", model.n_features_in_)\n",
    "print(\"Feature names in model:\", model.get_booster().feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
